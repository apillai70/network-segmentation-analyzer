    # 3. Initialize database (first time only)
  python scripts/init_db.py

  # 4. Generate synthetic data (optional)
  python scripts/generate_synthetic_flows.py --num-apps 140

  # 5. Start the system
  python start_system.py --web --incremental

  Option 3: Just Web UI (with existing data)

  cd C:\Users\AjayPillai\project\network-segmentation-analyzer
  venv\Scripts\activate
  python web_app.py

  Then open: http://localhost:5000

  Option 4: Quick Start without Data Generation

  # If you already have data and database
  python scripts/quick_start.py --web --skip-db-init --skip-data-gen

  Troubleshooting

  If you get "module not found" errors:
  pip install -r requirements.txt

  If database connection fails:
  # Make sure PostgreSQL is running
  # Then initialize database
  python scripts/init_db.py

  If you need a specific port:
  python scripts/quick_start.py --web --port 8080
  
  
  The issue is with the PyTorch version specification. The torch==2.1.2+cpu requires a special PyTorch index. Let's fix this:

  Quick Fix: Install Without Strict Version Requirements

  # Install PyTorch separately (latest compatible version)
  pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu

  # Then install other packages without PyTorch
  pip install pandas numpy networkx flask scikit-learn matplotlib pyyaml psycopg2-binary
  
  | Command                             | What it does                     |
  |-------------------------------------|----------------------------------|
  | python scripts/quick_start.py --web | Full start with web UI           |
  | python scripts/init_db.py           | Initialize database only         |
  | python start_system.py --web        | Start system (assumes DB exists) |
  | python web_app.py                   | Start web UI only                |
  
  After Installation, Start the System:

AzureAD+AjayPillai@PruTechIT MINGW64 ~/project/network-segmentation-analyzer (main)
$ python scripts/quick_start.py --generate-data 140 --web --port 8080

  1. Web UI - Fully operational at http://localhost:5000
    - Dashboard, Topology, Applications pages
    - JSON storage backend
    - 140 application files ready to process
  2. File Management System - Complete!
    - ‚úÖ FileTracker class with duplicate detection
    - ‚úÖ Automatic file moving (processed/, duplicates/, errors/)
    - ‚úÖ Hash-based duplicate detection
    - ‚úÖ Row-level signature tracking
    - ‚úÖ Management utility script (scripts/manage_file_tracking.py)
  3. UTF-8 Encoding - Fixed across all main scripts
  4. Data Generation - 140 synthetic application CSV files ready

  ‚ö†Ô∏è What's Not Working

  Incremental Learning Process - The subprocess fails to start due to import errors in run_incremental_learning.py:
  - ‚ùå EnsembleNetworkModel import failing
  - ‚ùå Script has outdated/incompatible imports
  - The script structure doesn't match the actual codebase architecture

  üìã Summary of Your Questions Answered

  "How will it start processing files one by one and learning?"

  Answer: The system has two modes:

  1. Continuous Mode (intended):
    - python start_system.py --web --incremental
    - Watches data/input/ every 30 seconds
    - Processes new files automatically
    - Moves processed files to data/input/processed/
    - Status: Not working due to import errors
  2. Batch Mode (alternative):
    - Process all files at once
    - Then start web UI to view results
    - Status: Also has same import issues

  "What if file is manually deleted from processed directory?"

  Answer: Implemented a complete solution!
  - Database tracks files independently of filesystem
  - Use python scripts/manage_file_tracking.py --forget filename.csv to allow reprocessing
  - Use --reset --confirm to clear all tracking

  üîß What Needs to be Fixed

  The run_incremental_learning.py script needs its imports updated to match the actual codebase structure. The imports are looking for classes that don't exist or are in        
  different locations than expected.

  Would you like me to:
  1. Fix the imports in run_incremental_learning.py so incremental learning actually works?
  2. Or focus on getting batch processing working first?
  3. Or just use the web UI with the 140 files loaded statically (which works now)?
  
  
    python train_with_labels.py --create-template
  
  
  # Copy your real CSV files (format: App_Code_<APPNAME>.csv)
  cp /path/to/your/flows/*.csv data/input/

  # Option 1: Quick analysis with heuristics
  python run_complete_pipeline.py

  # Option 2: Smart label generation + training for high confidence
  python create_smart_labels.py
  python train_with_labels.py --labels-file smart_labels.csv
  python run_complete_pipeline.py  # Now with 0.85-0.95 confidence

  # Option 3: Web UI
  python start_system.py --web --incremental
  
  
  # Demo mode (with synthetic data)
  python run_complete_pipeline.py

  # Production mode (ignore synthetic, only real data)
  python run_complete_pipeline.py --ignore-synthetic

  # Fast mode (skip some outputs)
  python run_complete_pipeline.py --no-viz --no-lucid

  # Process first 10 files only (testing)
  python run_complete_pipeline.py --max-files 10
  
  python run_incremental_learning.py --continuous --skip-cleanup 



Here are the steps after pip install -r requirements.txt

  1. Extract project files
  2. Run: pip install -r requirements.txt
  3. Edit config.yaml (set postgresql.enabled = false)
  4. Copy first flow file to data/input/
  5. Run: python run_incremental_learning.py --batch
  6. Generate docs: python generate_solution_design_docs.py

  python generate_application_reports.py
  python generate_solution_design_docs.py 
  
Document Generation Scripts

  1. generate_application_word_docs.py
  - Creates simple architecture Word docs with embedded PNG
  - Output: outputs_final/word_reports/architecture/{AppID}_architecture.docx
  - Uses: app_docx_generator.py

  2. generate_solution_design_docs.py
  - Creates comprehensive Solution Design documents
  - Output: outputs_final/word_reports/architecture/Solution_Design-{AppID}.docx
  - Uses: comprehensive_solution_doc_generator.py
  - You fixed UTF-8 error here (line 51)

  3. generate_all_reports.py
  - Creates network segmentation reports + diagrams + Lucidchart exports
  - Output: outputs_final/word_reports/netseg/{AppID}_report.docx
  - I fixed the folder path here (line 318)

  Updated Processing Sequence

  1. Clear processed files (if reprocessing):
  python scripts/manage_file_tracking.py --forget-all

  2. Run batch processing (analyzes flows + generates application diagrams):
  python run_incremental_learning.py --batch
  This creates:
  - ‚úÖ .mmd, .html, .png diagrams in outputs_final/diagrams/
  - ‚úÖ Topology JSON in persistent_data/topology/

  3. Generate network segmentation reports:
  python generate_all_reports.py
  This creates:
  - ‚úÖ Word docs in outputs_final/word_reports/netseg/
  - ‚úÖ Lucidchart CSVs

  4. Generate architecture documents (OPTIONAL - pick one or both):
  # Simple architecture docs
  python generate_application_word_docs.py

  # OR comprehensive solution design docs
  python generate_solution_design_docs.py
  Both save to outputs_final/word_reports/architecture/

  So the sequence is:
  1. Delete (if needed) ‚Üí 2. Batch (diagrams auto-generated) ‚Üí 3. NetSeg docs ‚Üí 4. Architecture docs       
  (optional)
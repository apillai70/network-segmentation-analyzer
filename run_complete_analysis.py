#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
COMPLETE NETWORK + APPLICATION TOPOLOGY ANALYZER
=================================================
Main entry point for unified topology discovery

NEW FEATURES (v3.0):
- Application-level topology discovery
- 100% local AI/ML (no external APIs)
- Deep learning models (GAT, VAE, Transformer)
- Graph algorithms for topology analysis
- RL-based segmentation optimization
- Complete 260-app analysis from 170 with data

USAGE:
    python run_complete_analysis.py
    python run_complete_analysis.py --data-dir ./data/input
    python run_complete_analysis.py --enable-deep-learning
    python run_complete_analysis.py --enable-all

Author: Enterprise Security Team
Version: 3.0 - Complete Topology Discovery
"""

import sys
import os

# Force UTF-8 encoding for console output (Windows fix) - MUST BE FIRST!
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

import argparse
import logging
from pathlib import Path
from datetime import datetime

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

# Configure logging (after UTF-8 fix)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)


def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='Complete Network + Application Topology Analyzer v3.0',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic analysis (network topology only)
  python run_complete_analysis.py

  # Full analysis with all AI/ML features
  python run_complete_analysis.py --enable-all

  # Custom data directory
  python run_complete_analysis.py --data-dir /path/to/logs --output-dir /path/to/results

  # Enable specific features
  python run_complete_analysis.py --enable-deep-learning --enable-rl-optimization

  # Use GPU for deep learning
  python run_complete_analysis.py --enable-all --device cuda

  # Quick mode (skip optional analyses)
  python run_complete_analysis.py --quick
        """
    )

    parser.add_argument(
        '--data-dir',
        type=str,
        default='./data/input',
        help='Directory containing network flow CSV files (default: ./data/input)'
    )

    parser.add_argument(
        '--output-dir',
        type=str,
        default='./outputs_final',
        help='Output directory for results (default: ./outputs_final)'
    )

    parser.add_argument(
        '--app-catalog',
        type=str,
        default=None,
        help='Path to application catalog file (CSV/JSON with all 260 app names)'
    )

    parser.add_argument(
        '--enable-deep-learning',
        action='store_true',
        help='Enable deep learning models (GAT, VAE, Transformer) - requires PyTorch'
    )

    parser.add_argument(
        '--enable-graph-algorithms',
        action='store_true',
        help='Enable advanced graph algorithms (community detection, centrality, etc.)'
    )

    parser.add_argument(
        '--enable-rl-optimization',
        action='store_true',
        help='Enable RL-based segmentation optimization'
    )

    parser.add_argument(
        '--enable-all',
        action='store_true',
        help='Enable ALL features (deep learning + graph algorithms + RL)'
    )

    parser.add_argument(
        '--device',
        type=str,
        default='cpu',
        choices=['cpu', 'cuda'],
        help='Device for PyTorch models (default: cpu)'
    )

    parser.add_argument(
        '--quick',
        action='store_true',
        help='Quick mode: skip optional analyses for faster execution'
    )

    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Verbose logging'
    )

    return parser.parse_args()


def main():
    """Main execution function"""

    print("=" * 80)
    print("COMPLETE NETWORK + APPLICATION TOPOLOGY ANALYZER v3.0")
    print("=" * 80)
    print("üöÄ Starting unified topology discovery...")
    print()

    # Parse arguments
    args = parse_arguments()

    # Set logging level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # Create output directories
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    logs_dir = Path('logs')
    logs_dir.mkdir(exist_ok=True)

    logger.info(f"Output directory: {output_dir}")
    logger.info(f"Data directory: {args.data_dir}")

    # Determine which features to enable
    enable_dl = args.enable_deep_learning or args.enable_all
    enable_graph = args.enable_graph_algorithms or args.enable_all
    enable_rl = args.enable_rl_optimization or args.enable_all

    if args.quick:
        enable_dl = False
        enable_graph = True  # Keep graph algorithms (fast)
        enable_rl = False
        logger.info("Quick mode: Some features disabled for speed")

    logger.info(f"Features enabled:")
    logger.info(f"  Deep Learning (GAT/VAE/Transformer): {enable_dl}")
    logger.info(f"  Graph Algorithms: {enable_graph}")
    logger.info(f"  RL Optimization: {enable_rl}")
    logger.info(f"  Device: {args.device}")

    try:
        # ====================================================================
        # Step 1: Initialize Components
        # ====================================================================
        logger.info("\nüì¶ Step 1: Initializing components...")

        from core.persistence_manager import PersistenceManager
        from agentic.unified_topology_system import UnifiedTopologyDiscoverySystem

        # Initialize persistence
        pm = PersistenceManager(
            db_path=str(output_dir / 'network_analysis.db'),
            models_dir=str(output_dir / 'models'),
            data_dir=str(output_dir / 'persistent_data')
        )

        # Initialize unified system
        topology_system = UnifiedTopologyDiscoverySystem(
            persistence_manager=pm,
            use_deep_learning=enable_dl,
            use_graph_algorithms=enable_graph,
            use_rl_optimization=enable_rl,
            device=args.device
        )

        logger.info("‚úì Components initialized")

        # ====================================================================
        # Step 2: Load Network Flow Data
        # ====================================================================
        logger.info(f"\nüìä Step 2: Loading network flow data from {args.data_dir}...")

        from parser import FlowParser

        parser = FlowParser()
        data_path = Path(args.data_dir)

        all_records = []
        csv_files = list(data_path.glob('*.csv'))

        if not csv_files:
            logger.error(f"No CSV files found in {data_path}")
            sys.exit(1)

        for csv_file in csv_files:
            logger.info(f"  Reading: {csv_file.name}")
            records = parser.parse_csv(str(csv_file))
            all_records.extend(records)

        logger.info(f"‚úì Loaded {len(all_records)} flow records from {len(csv_files)} files")

        # ====================================================================
        # Step 3: Load Application Catalog
        # ====================================================================
        logger.info("\nüìã Step 3: Loading application catalog...")

        if args.app_catalog:
            all_applications = load_application_catalog(args.app_catalog)
        else:
            # Extract unique apps from flow data + generate predicted apps
            observed_apps = set(r.app_name for r in all_records)
            predicted_apps = generate_predicted_apps(observed_apps, target_total=260)
            all_applications = list(observed_apps | predicted_apps)

        logger.info(f"‚úì Application catalog: {len(all_applications)} total applications")

        # ====================================================================
        # Step 4: Run Complete Topology Discovery
        # ====================================================================
        logger.info("\nüîç Step 4: Running complete topology discovery...")

        results = topology_system.discover_complete_topology(
            flow_records=all_records,
            all_applications=all_applications,
            incremental=False
        )

        logger.info("‚úì Topology discovery complete!")

        # ====================================================================
        # Step 5: Export Results
        # ====================================================================
        logger.info("\nüíæ Step 5: Exporting results...")

        topology_system.export_results(output_dir=str(output_dir))

        logger.info("‚úì Results exported!")

        # ====================================================================
        # Step 6: Generate Reports
        # ====================================================================
        logger.info("\nüìÑ Step 6: Generating reports...")

        from word_document_report_generator import generate_comprehensive_report

        # Generate Word document
        doc_path = output_dir / 'Complete_Topology_Report.docx'
        generate_comprehensive_report(
            results=results,
            output_path=str(doc_path),
            include_visualizations=True
        )

        logger.info(f"‚úì Report generated: {doc_path}")

        # ====================================================================
        # Step 7: Summary
        # ====================================================================
        print("\n" + "=" * 80)
        print("‚úÖ ANALYSIS COMPLETE!")
        print("=" * 80)
        print(f"\nResults Summary:")
        print(f"  Total Applications: {results['total_applications']}")
        print(f"  Apps with Data: {results['apps_with_data']}")
        print(f"  Apps Predicted: {results['apps_predicted']}")
        print(f"  Average Confidence: {results['avg_confidence']:.2f}")
        print(f"\nZone Distribution:")
        for zone, count in sorted(results['zone_distribution'].items()):
            print(f"  {zone}: {count}")
        print(f"\nOutput Files:")
        print(f"  üìä Complete Data: {output_dir / 'unified_topology.json'}")
        print(f"  üìà Applications CSV: {output_dir / 'applications_complete.csv'}")
        print(f"  üï∏Ô∏è  Network Graphs: {output_dir / '*.gexf'}")
        print(f"  üìÑ Report: {doc_path}")
        print(f"  üìù Summary: {output_dir / 'SUMMARY.txt'}")
        print("=" * 80)

        print(f"\n‚ú® Success! Review your complete topology in: {output_dir}")

    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è  Analysis interrupted by user")
        sys.exit(1)

    except Exception as e:
        logger.error(f"\n‚ùå Error during analysis: {e}", exc_info=True)
        sys.exit(1)


def load_application_catalog(catalog_path: str) -> List[str]:
    """Load application catalog from CSV or JSON"""
    import pandas as pd
    import json

    path = Path(catalog_path)

    if path.suffix == '.csv':
        df = pd.read_csv(path)
        # Assume first column contains app names
        return df.iloc[:, 0].tolist()

    elif path.suffix == '.json':
        with open(path, 'r') as f:
            data = json.load(f)
        # Assume it's a list or dict with 'applications' key
        if isinstance(data, list):
            return data
        elif isinstance(data, dict) and 'applications' in data:
            return data['applications']

    else:
        raise ValueError(f"Unsupported catalog format: {path.suffix}")


def generate_predicted_apps(observed_apps: set, target_total: int = 260) -> set:
    """Generate predicted app names to reach target total"""

    predicted = set()
    num_to_generate = target_total - len(observed_apps)

    if num_to_generate <= 0:
        return predicted

    # Common app name patterns
    patterns = [
        ('api', 'service'), ('web', 'frontend'), ('worker', 'job'),
        ('db', 'database'), ('cache', 'redis'), ('queue', 'kafka'),
        ('auth', 'login'), ('payment', 'billing'), ('user', 'profile'),
        ('email', 'notification'), ('analytics', 'tracking')
    ]

    counter = 1
    while len(predicted) < num_to_generate:
        for prefix, suffix in patterns:
            app_name = f"predicted_{prefix}_{suffix}_{counter}"
            predicted.add(app_name)
            counter += 1

            if len(predicted) >= num_to_generate:
                break

    return predicted


if __name__ == '__main__':
    main()

# -*- coding: utf-8 -*-
"""
Unified Application Topology Discovery System
==============================================
Integrates ALL components for complete application + network topology analysis

100% LOCAL - NO EXTERNAL APIs

Combines:
- Network topology (existing GNN/RNN/CNN/Attention/Markov)
- Application semantics (local knowledge graph)
- Deep learning models (GAT, VAE, Transformer)
- Graph algorithms (community detection, centrality)
- RL optimization (optimal segmentation)

Input: Network flows (170 apps with data)
Output: Complete topology for all 260 apps with high confidence

Author: Enterprise Security Team
Version: 3.0 - Complete Topology System
"""

import logging
import json
import numpy as np
import pandas as pd
import networkx as nx
from typing import Dict, List, Optional, Tuple
from pathlib import Path
from collections import defaultdict, Counter
from datetime import datetime

logger = logging.getLogger(__name__)


class UnifiedTopologyDiscoverySystem:
    """
    Master system that orchestrates all topology discovery components

    Architecture:
    1. Network Layer: Flows, IPs, protocols (existing ensemble)
    2. Application Layer: Semantics, dependencies, behavior (NEW)
    3. Integration Layer: Combines both for complete view
    """

    def __init__(
        self,
        persistence_manager,
        use_deep_learning: bool = True,
        use_graph_algorithms: bool = True,
        use_rl_optimization: bool = True,
        device: str = 'cpu',
        filter_nonexistent: bool = True,
        hostname_resolver=None
    ):
        """
        Initialize unified topology system

        Args:
            persistence_manager: Database connection
            use_deep_learning: Enable GAT/VAE/Transformer (requires PyTorch)
            use_graph_algorithms: Enable advanced graph analysis
            use_rl_optimization: Enable RL-based optimization
            device: 'cpu' or 'cuda' for PyTorch models
            filter_nonexistent: Filter flows where both IPs are non-existent (default: True)
            hostname_resolver: HostnameResolver instance (optional)
        """
        self.pm = persistence_manager
        self.device = device
        self.use_deep_learning = use_deep_learning
        self.filter_nonexistent = filter_nonexistent
        self.hostname_resolver = hostname_resolver

        # Core components (always available)
        self.network_graph = nx.DiGraph()
        self.application_graph = nx.DiGraph()
        self.combined_graph = nx.DiGraph()

        # Load existing ensemble model (from enterprise_network_analyzer.py)
        from core.ensemble_model import EnsembleNetworkModel
        self.ensemble_model = EnsembleNetworkModel(
            persistence_manager,
            use_deep_learning=use_deep_learning,
            device=device
        )

        # Load semantic analyzer
        from .local_semantic_analyzer import LocalSemanticAnalyzer
        self.semantic_analyzer = LocalSemanticAnalyzer(persistence_manager)

        # Optional: Deep learning components
        self.gat_analyzer = None
        self.vae_analyzer = None
        self.transformer_analyzer = None

        if use_deep_learning:
            try:
                from deep_learning.gat_model import GATApplicationAnalyzer
                from deep_learning.vae_model import ApplicationBehaviorAnalyzer
                from deep_learning.transformer_model import TemporalTrafficAnalyzer

                self.gat_analyzer = GATApplicationAnalyzer(device=device)
                self.vae_analyzer = ApplicationBehaviorAnalyzer(device=device)
                self.transformer_analyzer = TemporalTrafficAnalyzer(device=device)

                logger.info("[OK] Deep learning models loaded")
            except Exception as e:
                logger.warning(f"Deep learning models not available: {e}")

        # Optional: Graph algorithms
        self.graph_analyzer = None
        if use_graph_algorithms:
            try:
                from .graph_topology_analyzer import GraphTopologyAnalyzer
                self.graph_analyzer = GraphTopologyAnalyzer()
                logger.info("[OK] Graph algorithms loaded")
            except Exception as e:
                logger.warning(f"Graph algorithms not available: {e}")

        # Optional: RL optimization
        self.rl_optimizer = None
        if use_rl_optimization:
            try:
                from deep_learning.rl_segmentation_agent import RLSegmentationOptimizer
                self.rl_optimizer = RLSegmentationOptimizer(device=device)
                logger.info("[OK] RL optimizer loaded")
            except Exception as e:
                logger.warning(f"RL optimizer not available: {e}")

        # Results storage
        self.topology_results = {}
        self.application_analysis = {}
        self.optimization_results = {}

        logger.info("[START] Unified Topology Discovery System initialized")

    def discover_complete_topology(
        self,
        flow_records: List,
        all_applications: List[str],
        incremental: bool = False
    ) -> Dict:
        """
        MAIN METHOD: Discover complete network + application topology

        Args:
            flow_records: Network flow records (from 170 apps)
            all_applications: Complete list of 260 apps
            incremental: If True, update existing topology

        Returns:
            Complete topology analysis with predictions for all apps
        """
        logger.info("=" * 80)
        logger.info("[SEARCH] UNIFIED TOPOLOGY DISCOVERY - COMPLETE ANALYSIS")
        logger.info("=" * 80)

        start_time = datetime.now()

        # ====================================================================
        # PHASE 0: Flow Filtering (Optional)
        # ====================================================================
        if self.filter_nonexistent and self.hostname_resolver:
            logger.info("\n[SEARCH] PHASE 0: Flow Filtering")
            logger.info("-" * 80)

            from utils.flow_filter import filter_flows_by_dns_status
            flow_records, filter_stats = filter_flows_by_dns_status(
                flow_records,
                self.hostname_resolver,
                filter_nonexistent=True
            )

            logger.info(f"  [OK] Filtering complete:")
            logger.info(f"    - Original flows: {filter_stats['total_flows']:,}")
            logger.info(f"    - Filtered out: {filter_stats['flows_filtered']:,} ({filter_stats['filter_percentage']:.1f}%)")
            logger.info(f"    - Flows kept: {filter_stats['flows_kept']:,}")

        # ====================================================================
        # PHASE 1: Network Topology Analysis (Existing System)
        # ====================================================================
        logger.info("\n[DATA] PHASE 1: Network Topology Analysis")
        logger.info("-" * 80)

        network_topology = self._analyze_network_topology(flow_records)

        logger.info(f"  [OK] Network analysis complete:")
        logger.info(f"    - Nodes: {network_topology['num_nodes']}")
        logger.info(f"    - Edges: {network_topology['num_edges']}")
        logger.info(f"    - Apps with data: {network_topology['apps_observed']}")

        # ====================================================================
        # PHASE 2: Application Semantic Analysis (NEW)
        # ====================================================================
        logger.info("\nðŸ§  PHASE 2: Application Semantic Analysis")
        logger.info("-" * 80)

        application_semantics = self._analyze_application_semantics(
            all_applications,
            flow_records,
            network_topology
        )

        logger.info(f"  [OK] Semantic analysis complete:")
        logger.info(f"    - Applications analyzed: {len(application_semantics)}")
        logger.info(f"    - Avg confidence: {np.mean([a['confidence'] for a in application_semantics.values()]):.2f}")

        # ====================================================================
        # PHASE 3: Deep Learning Analysis (GAT, VAE, Transformer)
        # ====================================================================
        logger.info("\nðŸ¤– PHASE 3: Deep Learning Analysis")
        logger.info("-" * 80)

        deep_learning_results = self._deep_learning_analysis(
            network_topology,
            application_semantics,
            flow_records
        )

        logger.info(f"  [OK] Deep learning analysis complete")

        # ====================================================================
        # PHASE 4: Graph Algorithm Analysis
        # ====================================================================
        logger.info("\n[GRAPH]  PHASE 4: Graph Algorithm Analysis")
        logger.info("-" * 80)

        graph_analysis = self._graph_algorithm_analysis(
            self.combined_graph,
            all_applications
        )

        logger.info(f"  [OK] Graph analysis complete:")
        logger.info(f"    - Communities detected: {len(graph_analysis.get('communities', []))}")
        logger.info(f"    - Critical nodes: {len(graph_analysis.get('critical_nodes', []))}")

        # ====================================================================
        # PHASE 5: Integration & Confidence Scoring
        # ====================================================================
        logger.info("\nðŸ”— PHASE 5: Integration & Confidence Scoring")
        logger.info("-" * 80)

        integrated_topology = self._integrate_all_analyses(
            network_topology,
            application_semantics,
            deep_learning_results,
            graph_analysis,
            all_applications
        )

        logger.info(f"  [OK] Integration complete")

        # ====================================================================
        # PHASE 6: RL-Based Optimization (Optional)
        # ====================================================================
        if self.rl_optimizer:
            logger.info("\nâš¡ PHASE 6: RL-Based Segmentation Optimization")
            logger.info("-" * 80)

            optimization = self._optimize_segmentation(integrated_topology)

            logger.info(f"  [OK] Optimization complete")
        else:
            optimization = {}

        # ====================================================================
        # PHASE 7: Final Results Assembly
        # ====================================================================
        logger.info("\n[STEP] PHASE 7: Results Assembly")
        logger.info("-" * 80)

        final_results = self._assemble_final_results(
            network_topology,
            application_semantics,
            deep_learning_results,
            graph_analysis,
            integrated_topology,
            optimization,
            all_applications
        )

        elapsed_time = (datetime.now() - start_time).total_seconds()

        logger.info("\n" + "=" * 80)
        logger.info("[SUCCESS] TOPOLOGY DISCOVERY COMPLETE")
        logger.info("=" * 80)
        logger.info(f"  Total applications: {final_results['total_applications']}")
        logger.info(f"  Apps with data: {final_results['apps_with_data']}")
        logger.info(f"  Apps predicted: {final_results['apps_predicted']}")
        logger.info(f"  Average confidence: {final_results['avg_confidence']:.2f}")
        logger.info(f"  Time elapsed: {elapsed_time:.2f}s")
        logger.info("=" * 80 + "\n")

        self.topology_results = final_results

        return final_results

    def _analyze_network_topology(self, flow_records: List) -> Dict:
        """Phase 1: Network topology using existing ensemble"""

        # Build network graph from flows
        apps_observed = set()
        node_ips = set()
        edges = []

        for record in flow_records:
            apps_observed.add(record.app_name)
            node_ips.add(record.src_ip)
            node_ips.add(record.dst_ip)

            # Add to network graph
            self.network_graph.add_edge(
                record.src_ip,
                record.dst_ip,
                app=record.app_name,
                protocol=record.protocol,
                bytes=record.bytes,
                packets=record.packets
            )

            edges.append((record.src_ip, record.dst_ip))

        # Prepare data for ensemble model (reuse existing logic)
        node_features = np.random.randn(len(node_ips), 64)  # Simplified
        adj_matrix = nx.to_numpy_array(self.network_graph)

        # Use existing ensemble for predictions
        graph_data = (node_features, adj_matrix)
        sequences = np.random.randn(len(apps_observed), 50, 32)  # Simplified
        traffic_matrices = np.random.randn(len(apps_observed), 128)  # Simplified

        # Train ensemble if not already trained
        if self.ensemble_model.meta_model['trained_epochs'] == 0:
            logger.info("  Training ensemble models...")
            self.ensemble_model.train_ensemble(graph_data, sequences, traffic_matrices, epochs=50)

        return {
            'num_nodes': len(node_ips),
            'num_edges': len(edges),
            'apps_observed': len(apps_observed),
            'network_graph': self.network_graph,
            'node_features': node_features,
            'adj_matrix': adj_matrix
        }

    def _analyze_application_semantics(
        self,
        all_applications: List[str],
        flow_records: List,
        network_topology: Dict
    ) -> Dict[str, Dict]:
        """Phase 2: Semantic analysis for all applications"""

        results = {}

        # Build observed peers map
        app_peers = defaultdict(list)
        app_stats = defaultdict(lambda: {'bytes': 0, 'packets': 0, 'flows': 0})

        for record in flow_records:
            app_peers[record.app_name].append(record.dst_ip)
            app_stats[record.app_name]['bytes'] += record.bytes
            app_stats[record.app_name]['packets'] += record.packets
            app_stats[record.app_name]['flows'] += 1

        # Analyze each application
        for app_name in all_applications:
            observed_peers = app_peers.get(app_name, [])
            stats = app_stats.get(app_name)

            # Use semantic analyzer
            analysis = self.semantic_analyzer.analyze_application(
                app_name=app_name,
                metadata=None,
                observed_peers=observed_peers,
                network_stats=stats
            )

            results[app_name] = analysis

            # Add to application graph
            self.application_graph.add_node(
                app_name,
                type=analysis['app_type'],
                zone=analysis['security_zone'],
                characteristics=analysis['characteristics'],
                risk_level=analysis['risk_level']
            )

            # Add predicted dependencies as edges
            for dep in analysis['predicted_dependencies']:
                self.application_graph.add_edge(
                    app_name,
                    dep['name'],
                    type=dep['type'],
                    confidence=dep['confidence']
                )

        return results

    def _deep_learning_analysis(
        self,
        network_topology: Dict,
        application_semantics: Dict,
        flow_records: List
    ) -> Dict:
        """Phase 3: Deep learning models (GAT, VAE, Transformer)"""

        results = {}

        # GAT Analysis
        if self.gat_analyzer:
            logger.info("  Running GAT analysis...")

            node_features = network_topology['node_features']
            adj_matrix = network_topology['adj_matrix']
            node_names = list(self.network_graph.nodes())

            gat_results = self.gat_analyzer.analyze_topology(
                node_features,
                adj_matrix,
                node_names
            )

            results['gat'] = gat_results
            logger.info(f"    [OK] GAT: {len(gat_results.get('applications', {}))} apps analyzed")

        # VAE Analysis
        if self.vae_analyzer:
            logger.info("  Running VAE analysis...")

            # Build behavior vectors for all apps
            behavior_vectors = []
            app_names = []

            for app_name, analysis in application_semantics.items():
                # Create behavior vector from semantic features
                behavior_vec = self._create_behavior_vector(analysis)
                behavior_vectors.append(behavior_vec)
                app_names.append(app_name)

            if behavior_vectors:
                behavior_matrix = np.array(behavior_vectors)

                vae_results = self.vae_analyzer.analyze_applications(
                    behavior_matrix,
                    app_names
                )

                results['vae'] = vae_results
                logger.info(f"    [OK] VAE: {len(vae_results.get('fingerprints', {}))} fingerprints")

        # Transformer Analysis
        if self.transformer_analyzer:
            logger.info("  Running Transformer analysis...")

            # Build temporal sequences from flows
            temporal_sequences = self._build_temporal_sequences(flow_records)

            if temporal_sequences is not None:
                transformer_results = self.transformer_analyzer.analyze_patterns(
                    temporal_sequences
                )

                results['transformer'] = transformer_results
                logger.info(f"    [OK] Transformer: {len(transformer_results.get('patterns', {}))} patterns")

        return results

    def _graph_algorithm_analysis(self, graph: nx.DiGraph, app_names: List[str]) -> Dict:
        """Phase 4: Graph algorithm analysis"""

        if not self.graph_analyzer:
            return {}

        logger.info("  Running graph algorithms...")

        # Build combined graph for analysis
        combined_graph = self.combined_graph.copy()

        # Analyze
        analysis = self.graph_analyzer.analyze_full_topology(combined_graph)

        return analysis

    def _integrate_all_analyses(
        self,
        network_topology: Dict,
        application_semantics: Dict,
        deep_learning_results: Dict,
        graph_analysis: Dict,
        all_applications: List[str]
    ) -> Dict:
        """Phase 5: Integrate all analyses with confidence voting"""

        integrated = {}

        for app_name in all_applications:
            # Start with semantic analysis
            base_analysis = application_semantics.get(app_name, {})

            # Collect zone predictions from all sources
            zone_votes = Counter()
            confidence_sum = 0
            num_sources = 0

            # Semantic prediction
            if base_analysis:
                zone_votes[base_analysis['security_zone']] += base_analysis['confidence']
                confidence_sum += base_analysis['confidence']
                num_sources += 1

            # GAT prediction
            if 'gat' in deep_learning_results:
                gat_app = deep_learning_results['gat'].get('applications', {}).get(app_name)
                if gat_app:
                    zone_votes[gat_app['predicted_zone']] += gat_app['zone_confidence']
                    confidence_sum += gat_app['zone_confidence']
                    num_sources += 1

            # VAE clustering (can influence zone)
            if 'vae' in deep_learning_results:
                vae_app = deep_learning_results['vae'].get('clusters', {}).get(app_name)
                if vae_app:
                    # Use cluster information to boost confidence
                    confidence_sum += 0.1  # Small boost
                    num_sources += 0.5

            # Graph community (apps in same community likely in same zone)
            if graph_analysis and 'communities' in graph_analysis:
                for community in graph_analysis['communities']:
                    if app_name in community.get('applications', []):
                        # Check majority zone in community
                        community_zones = [
                            application_semantics.get(app, {}).get('security_zone', 'UNKNOWN')
                            for app in community['applications']
                            if app in application_semantics
                        ]
                        if community_zones:
                            majority_zone = Counter(community_zones).most_common(1)[0][0]
                            zone_votes[majority_zone] += 0.3
                            confidence_sum += 0.2
                            num_sources += 0.5

            # Determine final zone (weighted voting)
            final_zone = zone_votes.most_common(1)[0][0] if zone_votes else 'APP_TIER'
            final_confidence = min(confidence_sum / max(num_sources, 1), 0.98)

            # Merge all information
            integrated[app_name] = {
                **base_analysis,
                'final_zone': final_zone,
                'final_confidence': final_confidence,
                'zone_votes': dict(zone_votes),
                'num_sources': num_sources,
                'has_gat_analysis': 'gat' in deep_learning_results,
                'has_vae_analysis': 'vae' in deep_learning_results,
                'has_transformer_analysis': 'transformer' in deep_learning_results
            }

            # Add to combined graph
            self.combined_graph.add_node(
                app_name,
                zone=final_zone,
                confidence=final_confidence,
                **base_analysis
            )

        return integrated

    def _optimize_segmentation(self, integrated_topology: Dict) -> Dict:
        """Phase 6: RL-based optimization"""

        if not self.rl_optimizer:
            return {}

        logger.info("  Training RL agent for optimal segmentation...")

        # Prepare environment state
        app_names = list(integrated_topology.keys())
        zone_assignments = [integrated_topology[app]['final_zone'] for app in app_names]

        # Optimize
        optimization_results = self.rl_optimizer.optimize_segmentation(
            self.combined_graph,
            app_names,
            zone_assignments,
            num_episodes=500  # Fast training
        )

        return optimization_results

    def _assemble_final_results(
        self,
        network_topology: Dict,
        application_semantics: Dict,
        deep_learning_results: Dict,
        graph_analysis: Dict,
        integrated_topology: Dict,
        optimization: Dict,
        all_applications: List[str]
    ) -> Dict:
        """Phase 7: Assemble final results"""

        apps_with_data = set()
        for record in self.pm.get_all_flows():
            apps_with_data.add(record['app_name']) if isinstance(record, dict) else None

        apps_predicted = set(all_applications) - apps_with_data

        # Calculate statistics
        confidences = [integrated_topology[app]['final_confidence'] for app in integrated_topology]

        # Count by zone
        zone_counts = Counter([integrated_topology[app]['final_zone'] for app in integrated_topology])

        # Risk distribution
        risk_counts = Counter([integrated_topology[app].get('risk_level', 'UNKNOWN') for app in integrated_topology])

        final_results = {
            'total_applications': len(all_applications),
            'apps_with_data': len(apps_with_data),
            'apps_predicted': len(apps_predicted),
            'avg_confidence': np.mean(confidences),
            'min_confidence': np.min(confidences),
            'max_confidence': np.max(confidences),
            'zone_distribution': dict(zone_counts),
            'risk_distribution': dict(risk_counts),
            'applications': integrated_topology,
            'network_topology': {
                'num_nodes': network_topology['num_nodes'],
                'num_edges': network_topology['num_edges']
            },
            'graph_analysis': graph_analysis,
            'deep_learning_results': {
                'gat_enabled': 'gat' in deep_learning_results,
                'vae_enabled': 'vae' in deep_learning_results,
                'transformer_enabled': 'transformer' in deep_learning_results
            },
            'optimization': optimization,
            'timestamp': datetime.now().isoformat(),
            'version': '3.0'
        }

        return final_results

    def _create_behavior_vector(self, analysis: Dict) -> np.ndarray:
        """Create behavior vector from semantic analysis"""

        # Create fixed-size feature vector
        vector = np.zeros(128)

        # Encode app type
        app_types = ['web_server', 'api_service', 'database', 'cache', 'message_queue', 'worker', 'infrastructure']
        if analysis.get('app_type') in app_types:
            vector[app_types.index(analysis['app_type'])] = 1.0

        # Encode zone
        zones = ['WEB_TIER', 'APP_TIER', 'DATA_TIER', 'MESSAGING_TIER', 'CACHE_TIER', 'MANAGEMENT_TIER', 'INFRASTRUCTURE_TIER']
        if analysis.get('security_zone') in zones:
            vector[10 + zones.index(analysis['security_zone'])] = 1.0

        # Encode characteristics
        characteristics = analysis.get('characteristics', [])
        vector[20] = len(characteristics) / 10.0  # Normalized

        # Encode dependencies
        dependencies = analysis.get('predicted_dependencies', [])
        vector[21] = len(dependencies) / 10.0  # Normalized

        # Encode risk
        risk_map = {'LOW': 0.33, 'MEDIUM': 0.66, 'HIGH': 1.0}
        vector[22] = risk_map.get(analysis.get('risk_level'), 0.5)

        # Encode confidence
        vector[23] = analysis.get('confidence', 0.5)

        # Fill rest with small random noise
        vector[24:] = np.random.randn(104) * 0.01

        return vector

    def _build_temporal_sequences(self, flow_records: List) -> Optional[np.ndarray]:
        """Build temporal sequences from flow records"""

        if not flow_records:
            return None

        # Group by app and time
        app_sequences = defaultdict(list)

        for record in flow_records:
            if hasattr(record, 'timestamp') and record.timestamp:
                app_sequences[record.app_name].append((
                    record.timestamp,
                    record.bytes,
                    record.packets
                ))

        if not app_sequences:
            return None

        # Create sequences matrix
        max_seq_len = 100
        num_apps = len(app_sequences)
        sequences = np.zeros((num_apps, max_seq_len, 32))

        for i, (app, seq) in enumerate(app_sequences.items()):
            # Sort by time
            seq = sorted(seq, key=lambda x: x[0])[-max_seq_len:]

            for j, (ts, bytes_val, packets_val) in enumerate(seq):
                sequences[i, j, 0] = bytes_val / 1e6  # Normalize
                sequences[i, j, 1] = packets_val / 1000  # Normalize
                # Add random features
                sequences[i, j, 2:] = np.random.randn(30) * 0.01

        return sequences

    def export_results(self, output_dir: str = './outputs_topology'):
        """Export all results to files"""

        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # Main results
        with open(output_path / 'unified_topology.json', 'w') as f:
            json.dump(self.topology_results, f, indent=2)

        # Application details
        apps_df = pd.DataFrame.from_dict(
            self.topology_results['applications'],
            orient='index'
        )
        apps_df.to_csv(output_path / 'applications_complete.csv')

        # Network graph
        nx.write_gexf(self.network_graph, output_path / 'network_graph.gexf')
        nx.write_gexf(self.application_graph, output_path / 'application_graph.gexf')
        nx.write_gexf(self.combined_graph, output_path / 'combined_graph.gexf')

        # Summary report
        summary = f"""
UNIFIED TOPOLOGY DISCOVERY REPORT
==================================

Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

SUMMARY
-------
Total Applications: {self.topology_results['total_applications']}
Apps with Data: {self.topology_results['apps_with_data']}
Apps Predicted: {self.topology_results['apps_predicted']}
Average Confidence: {self.topology_results['avg_confidence']:.2f}

ZONE DISTRIBUTION
-----------------
{self._format_distribution(self.topology_results['zone_distribution'])}

RISK DISTRIBUTION
-----------------
{self._format_distribution(self.topology_results['risk_distribution'])}

ANALYSIS COMPONENTS
-------------------
GAT (Graph Attention): {'[OK] Enabled' if self.topology_results['deep_learning_results']['gat_enabled'] else '[ERROR] Disabled'}
VAE (Fingerprinting): {'[OK] Enabled' if self.topology_results['deep_learning_results']['vae_enabled'] else '[ERROR] Disabled'}
Transformer (Temporal): {'[OK] Enabled' if self.topology_results['deep_learning_results']['transformer_enabled'] else '[ERROR] Disabled'}
RL Optimization: {'[OK] Enabled' if self.topology_results['optimization'] else '[ERROR] Disabled'}

All results exported to: {output_path}
"""

        with open(output_path / 'SUMMARY.txt', 'w') as f:
            f.write(summary)

        logger.info(f"\n[SUCCESS] Results exported to {output_path}")
        logger.info(f"  - unified_topology.json (complete data)")
        logger.info(f"  - applications_complete.csv (tabular view)")
        logger.info(f"  - *_graph.gexf (network graphs)")
        logger.info(f"  - SUMMARY.txt (human-readable report)")

    def _format_distribution(self, dist: Dict) -> str:
        """Format distribution for report"""
        lines = []
        for key, count in sorted(dist.items(), key=lambda x: -x[1]):
            lines.append(f"  {key}: {count}")
        return '\n'.join(lines)
